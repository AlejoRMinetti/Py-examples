{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_Estructura_Redes_Neuronales.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlejoRMinetti/keras-scikit-examples/blob/main/3_Estructura_Redes_Neuronales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iFOX_Y3igEV"
      },
      "source": [
        "![texto alternativo](https://drive.google.com/uc?id=1_ZoJOp39dxsRTVndJP1_7jdY7oGi8sSu) \n",
        "\n",
        "<h1><center>Redes Neuronales con Keras y Scikit</center></h1>\n",
        "\n",
        "\n",
        "<center><i>Estructura de Redes Neuronales</i></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARGQZBuPwJoD"
      },
      "source": [
        "# <h1>Contenido</h1>\n",
        "\n",
        "<div class=\"alert alert-block alert-warning\" style=\"margin-top: 20px\">\n",
        "    <ol>\n",
        "        <li><a href=\"#perceptron\">Perceptrón</a></li>          \n",
        "        <li><a href=\"#iris\">Dataset IRIS</a></li>\n",
        "        <li><a href=\"#mlp\">Multi Layer Perceptron (MLP)</a></li>\n",
        "        <li><a href=\"#backward\">Backward Propagation</a></li>\n",
        "    </ol>\n",
        "</div>\n",
        "<br>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VfqD089-WuQ"
      },
      "source": [
        "# <h1 id=\"perceptron\">Perceptrón</h1>\n",
        "\n",
        "Representación más simple de una neurona, su salida es de característica binaria (1, 0) y depende de una combinación lineal de las señales de entrada.\n",
        "\n",
        "![Perceptrón](https://drive.google.com/uc?id=1Q6jJ75c_ywpPdJlYWxqAFRDWJHhcOyXo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRC6JT-P_l6Y"
      },
      "source": [
        "# <h1 id=\"iris\">Dataset IRIS</h1>\n",
        "\n",
        "El Famoso set de datos [IRIS](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html) contiene información de 3 diferentes especies (target):\n",
        "\n",
        "* Iris-Setosa (0)\n",
        "* Iris-Versicolor (1)\n",
        "* Iris-Virginica (2)\n",
        "\n",
        "El dataset contiene 150 muestras (50 por cada especie) y 4 características:\n",
        "\n",
        "* Longitud del sepalo (cm)\n",
        "* Ancho del sepalo (cm)\n",
        "* Longitud del petalo (cm)\n",
        "* Ancho del petalo (cm)\n",
        "\n",
        "![IRIS](https://drive.google.com/uc?id=1yUmL-JySS8ZyI9u1lFksK6ERR2AXRJfK)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLkXmSMOB7De"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "iris = load_iris()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFIwLRI-CzwG"
      },
      "source": [
        "iris.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZT795N7DLlm"
      },
      "source": [
        "iris.data[:5, :] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOv9TAouJMDX"
      },
      "source": [
        "## Gráfiquemos dos variables: longitud y ancho del pétalo ##\n",
        "data = iris.data[:, (2, 3)]\n",
        "labels = iris.target\n",
        "\n",
        "plt.figure(figsize=(13,6))\n",
        "plt.scatter(data[:, 0], data[:, 1], c=labels, cmap=plt.cm.Set1, edgecolor='face')\n",
        "plt.xlabel('Longitud pétalo (cm)')\n",
        "plt.ylabel('Ancho pétalo (cm)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEHrwO2v8tnL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "759aa311-816f-46e4-a05b-e2e932f2b7e8"
      },
      "source": [
        "## Longitud y ancho del petalo ##\n",
        "X = iris.data[:, (2, 3)] \n",
        "## Es Iris Virginica?\n",
        "y = (iris.target == 2).astype(np.int) \n",
        "\n",
        "test_perceptron = Perceptron()\n",
        "test_perceptron.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
              "           fit_intercept=True, max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
              "           penalty=None, random_state=0, shuffle=True, tol=0.001,\n",
              "           validation_fraction=0.1, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxzLm5_e88ro",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50916e55-682f-4ca6-f404-32de5a426b0a"
      },
      "source": [
        "y1_pred = test_perceptron.predict([[5.1, 2]])\n",
        "y2_pred = test_perceptron.predict([[1.4, 0.2]])\n",
        "print('Predicción 1:', y1_pred, 'Predicción 2:', y2_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicción 1: [1] Predicción 2: [0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iVYjQhSHfTk"
      },
      "source": [
        "El Perceptrón permite implementar clasificadores binarios muy simples, sin embargo a la hora de enfrentar problemas más complejos como la identificación de patrones se recurre a combinar múltiples perceptrones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxvW1nUuF7Fd"
      },
      "source": [
        "# <h1 id=\"mlp\">Multi-layer Perceptron (MLP)</h1>\n",
        "\n",
        "\n",
        "![texto alternativo](https://drive.google.com/uc?id=12-AUUKOswfzkZW6xRziIZhxldCa9vDSU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDEjLOCmSJiE"
      },
      "source": [
        "Una arquitectura de múltiples capas de perceptrones o MLP) es la respuesta a la necesidad de poder analizar problemas más complejos como lo es la clasificación de imágenes. A grandes rasgos y tal y como se muestra en la animación está compuesta por una capa de entrada, una o múltiples capas de TLUs denominadas capas ocultas cada una compuesta por diferentes perceptrones y una capa final denominada capa de salida. Cuando se incluyen más de dos capas ocultas se suele denominar red neuronal profunda (Deep Neural Network DNN)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5KmY1jyY95f"
      },
      "source": [
        "![texto alternativo](https://drive.google.com/uc?id=1n0EQyBxXsxCBFQWTZJl-BbJIN2_Ht6sv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXqyslIFeCGZ"
      },
      "source": [
        "# <h1 id=\"backward\">Backward Propagation</h1>\n",
        "\n",
        "La propagación hacia atrás o backpropagation es un algoritmo de entrenamiento de redes neuronales. El método consiste en un ciclo de propagación y adaptación en dos fases. Primero se aplica un estímulo a la entrada de la red que se propaga por todas las capas hasta generar una salida. Luego la señal de salida se compara con la salida deseada y se calcula el error para cada una de las salidas. Enseguida, el error calculado se propaga hacia atrás partiendo desde la capa de salida hasta la capa de entrada de tal manera que todas las neuronas reciben una retroalimentación respecto a su contribución relativa al error. \n",
        "\n",
        "![texto alternativo](https://drive.google.com/uc?id=1YC5Lcj-Axj0XewPZ7glzbIIOZbdCY219)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9Nlfrn5w-jC"
      },
      "source": [
        "## Algoritmo Backward propagation\n",
        "\n",
        "* Se procesa un mini-lote de información a la vez. Cada paso de la información por la red se denomina época. \n",
        "*  Cada mini-batch de información es pasado por la capa de entrada la cual lo envía a la primera capa oculta. \n",
        "*  Luego se procesa la salida de todas las neuronas en esta capa (por cada muestra en el batch) \n",
        "*  El resultado se pasa a la siguiente capa y así sucesivamente hasta la capa de salida. Hasta ahora el proceso es el mismo que forward pass, todos los resultados intermedios son almacenados hasta que son necesitados por el paso de backward.\n",
        "*  Enseguida el algoritmo mide el error en la salida mediante funciones de pérdidas y retorna la medida del error.\n",
        "*  Luego se calcula la contribución en el error de conexión correspondiente con la capa anterior aplicando la regla de la cadena, fundamental en el cálculo. Este proceso se reitera hasta la capa de entrada.\n",
        "*  En efecto el algoritmo propaga el gradiente del error a todos los pesos correspondientes de las conexiones en la red.\n",
        "*  Finalmente se ejecuta un gradiente descendente para ajustar todos los pesos sinápticos en la red empleando los gradientes de error.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okn48pX5531G"
      },
      "source": [
        "**Happy Coding!**"
      ]
    }
  ]
}